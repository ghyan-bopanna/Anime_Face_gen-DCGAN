# -*- coding: utf-8 -*-
"""Copy of anime_face_GAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15QOIGuAze1_fdBH11igremRHzbJ2fnZW
"""

# Anime Face Generation using DCGAN
# This notebook implements a Deep Convolutional GAN for generating anime faces

# ============================================================================
# SECTION 1: SETUP AND IMPORTS
# ============================================================================

import os
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import time
import json

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision
import torchvision.transforms as transforms
from torchvision.utils import save_image, make_grid

from PIL import Image
from pathlib import Path
import zipfile

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if device.type == 'cuda':
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# ============================================================================
# SECTION 2: HYPERPARAMETERS
# ============================================================================

# Training hyperparameters
BATCH_SIZE = 128
NUM_EPOCHS = 100
LEARNING_RATE_G = 0.0002
LEARNING_RATE_D = 0.0002
BETA1 = 0.5
BETA2 = 0.999

# Model hyperparameters
LATENT_DIM = 100
IMAGE_SIZE = 64
IMAGE_CHANNELS = 3
FEATURE_MAP_G = 64
FEATURE_MAP_D = 64

# Training configuration
DISCRIMINATOR_STEPS = 1  # Number of D updates per G update
LABEL_SMOOTHING = 0.1  # One-sided label smoothing for real images
SAVE_INTERVAL = 5  # Save model every N epochs
SAMPLE_INTERVAL = 1  # Generate samples every N epochs

# Paths
DATASET_PATH = '/content/anime_faces'
OUTPUT_DIR = '/content/gan_output'
CHECKPOINT_DIR = f'{OUTPUT_DIR}/checkpoints'
SAMPLES_DIR = f'{OUTPUT_DIR}/samples'

# Create directories
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
os.makedirs(SAMPLES_DIR, exist_ok=True)

# ============================================================================
# SECTION 3: DATASET SETUP
# ============================================================================

# Download dataset using kagglehub
import kagglehub
path = kagglehub.dataset_download("splcher/animefacedataset")
print("Path to dataset files:", path)
DATASET_PATH = path  # Update the dataset path

class AnimeFaceDataset(Dataset):
    """Custom Dataset for loading anime face images"""

    def __init__(self, root_dir, transform=None):
        self.root_dir = Path(root_dir)
        self.transform = transform

        # Get all image files
        self.image_paths = []
        for ext in ['*.jpg', '*.jpeg', '*.png']:
            self.image_paths.extend(list(self.root_dir.glob(ext)))
            self.image_paths.extend(list(self.root_dir.glob(f'**/{ext}')))

        print(f"Found {len(self.image_paths)} images")

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image

# Data preprocessing and augmentation
transform = transforms.Compose([
    transforms.Resize(IMAGE_SIZE),
    transforms.CenterCrop(IMAGE_SIZE),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]
])

# Create dataset and dataloader
try:
    dataset = AnimeFaceDataset(DATASET_PATH, transform=transform)
    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=2,
        pin_memory=True if device.type == 'cuda' else False,
        drop_last=True
    )
    print(f"Dataset ready with {len(dataset)} images")
    print(f"Number of batches per epoch: {len(dataloader)}")
except Exception as e:
    print(f"Error loading dataset: {e}")
    print("Please ensure the dataset is downloaded and extracted correctly")

# ============================================================================
# SECTION 4: MODEL ARCHITECTURE (DCGAN)
# ============================================================================

def weights_init(m):
    """Initialize network weights (DCGAN paper recommendations)"""
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

class Generator(nn.Module):
    """DCGAN Generator Network"""

    def __init__(self, latent_dim=100, feature_maps=64, img_channels=3):
        super(Generator, self).__init__()

        self.main = nn.Sequential(
            # Input: latent_dim x 1 x 1
            nn.ConvTranspose2d(latent_dim, feature_maps * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(feature_maps * 8),
            nn.ReLU(True),
            # State: (feature_maps*8) x 4 x 4

            nn.ConvTranspose2d(feature_maps * 8, feature_maps * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 4),
            nn.ReLU(True),
            # State: (feature_maps*4) x 8 x 8

            nn.ConvTranspose2d(feature_maps * 4, feature_maps * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 2),
            nn.ReLU(True),
            # State: (feature_maps*2) x 16 x 16

            nn.ConvTranspose2d(feature_maps * 2, feature_maps, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps),
            nn.ReLU(True),
            # State: feature_maps x 32 x 32

            nn.ConvTranspose2d(feature_maps, img_channels, 4, 2, 1, bias=False),
            nn.Tanh()
            # Output: img_channels x 64 x 64
        )

    def forward(self, z):
        return self.main(z)

class Discriminator(nn.Module):
    """DCGAN Discriminator Network"""

    def __init__(self, img_channels=3, feature_maps=64):
        super(Discriminator, self).__init__()

        self.main = nn.Sequential(
            # Input: img_channels x 64 x 64
            nn.Conv2d(img_channels, feature_maps, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # State: feature_maps x 32 x 32

            nn.Conv2d(feature_maps, feature_maps * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # State: (feature_maps*2) x 16 x 16

            nn.Conv2d(feature_maps * 2, feature_maps * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # State: (feature_maps*4) x 8 x 8

            nn.Conv2d(feature_maps * 4, feature_maps * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_maps * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # State: (feature_maps*8) x 4 x 4

            nn.Conv2d(feature_maps * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
            # Output: 1 x 1 x 1
        )

    def forward(self, img):
        return self.main(img).view(-1, 1)

# Initialize models
generator = Generator(LATENT_DIM, FEATURE_MAP_G, IMAGE_CHANNELS).to(device)
discriminator = Discriminator(IMAGE_CHANNELS, FEATURE_MAP_D).to(device)

# Apply weight initialization
generator.apply(weights_init)
discriminator.apply(weights_init)

# Print model architectures
print("="*70)
print("GENERATOR ARCHITECTURE:")
print("="*70)
print(generator)
print(f"\nTotal parameters: {sum(p.numel() for p in generator.parameters()):,}")

print("\n" + "="*70)
print("DISCRIMINATOR ARCHITECTURE:")
print("="*70)
print(discriminator)
print(f"\nTotal parameters: {sum(p.numel() for p in discriminator.parameters()):,}")

# ============================================================================
# SECTION 5: LOSS FUNCTION AND OPTIMIZERS
# ============================================================================

# Binary Cross Entropy Loss
criterion = nn.BCELoss()

# Optimizers (using Adam as recommended in DCGAN paper)
optimizer_G = optim.Adam(
    generator.parameters(),
    lr=LEARNING_RATE_G,
    betas=(BETA1, BETA2)
)

optimizer_D = optim.Adam(
    discriminator.parameters(),
    lr=LEARNING_RATE_D,
    betas=(BETA1, BETA2)
)

print("\n" + "="*70)
print("TRAINING CONFIGURATION:")
print("="*70)
print(f"Batch Size: {BATCH_SIZE}")
print(f"Number of Epochs: {NUM_EPOCHS}")
print(f"Learning Rate (G): {LEARNING_RATE_G}")
print(f"Learning Rate (D): {LEARNING_RATE_D}")
print(f"Latent Dimension: {LATENT_DIM}")
print(f"Image Size: {IMAGE_SIZE}x{IMAGE_SIZE}")

# ============================================================================
# SECTION 6: UTILITY FUNCTIONS
# ============================================================================

def generate_samples(generator, num_samples=64, latent_dim=100, save_path=None):
    """Generate and display sample images"""
    generator.eval()
    with torch.no_grad():
        z = torch.randn(num_samples, latent_dim, 1, 1).to(device)
        fake_images = generator(z)

        # Denormalize images from [-1, 1] to [0, 1]
        fake_images = (fake_images + 1) / 2.0

        if save_path:
            save_image(fake_images, save_path, nrow=8, normalize=False)

        # Display images
        grid = make_grid(fake_images, nrow=8, normalize=False)
        plt.figure(figsize=(12, 12))
        plt.imshow(grid.cpu().permute(1, 2, 0))
        plt.axis('off')
        plt.title(f'Generated Anime Faces')
        plt.tight_layout()
        plt.show()

    generator.train()

def plot_losses(g_losses, d_losses, save_path=None):
    """Plot training losses"""
    plt.figure(figsize=(10, 5))
    plt.plot(g_losses, label='Generator Loss', alpha=0.7)
    plt.plot(d_losses, label='Discriminator Loss', alpha=0.7)
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Training Losses')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150)

    plt.show()

def save_checkpoint(epoch, generator, discriminator, optimizer_G, optimizer_D,
                    g_losses, d_losses, checkpoint_dir):
    """Save model checkpoint"""
    checkpoint_path = f'{checkpoint_dir}/checkpoint_epoch_{epoch}.pt'
    torch.save({
        'epoch': epoch,
        'generator_state_dict': generator.state_dict(),
        'discriminator_state_dict': discriminator.state_dict(),
        'optimizer_G_state_dict': optimizer_G.state_dict(),
        'optimizer_D_state_dict': optimizer_D.state_dict(),
        'g_losses': g_losses,
        'd_losses': d_losses,
    }, checkpoint_path)
    print(f"Checkpoint saved: {checkpoint_path}")

def load_checkpoint(checkpoint_path, generator, discriminator, optimizer_G, optimizer_D):
    """Load model checkpoint"""
    checkpoint = torch.load(checkpoint_path)
    generator.load_state_dict(checkpoint['generator_state_dict'])
    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])
    optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])
    optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])

    return (checkpoint['epoch'],
            checkpoint['g_losses'],
            checkpoint['d_losses'])

# ============================================================================
# SECTION 7: TRAINING LOOP
# ============================================================================

def train_gan(generator, discriminator, dataloader, num_epochs,
              optimizer_G, optimizer_D, criterion, device):
    """Main training function for the GAN"""

    # Lists to store losses
    g_losses = []
    d_losses = []

    # Fixed noise for consistent sample generation
    fixed_noise = torch.randn(64, LATENT_DIM, 1, 1).to(device)

    # Real and fake labels
    real_label = 1.0 - LABEL_SMOOTHING  # One-sided label smoothing
    fake_label = 0.0

    print("\n" + "="*70)
    print("STARTING TRAINING")
    print("="*70)

    start_time = time.time()

    for epoch in range(num_epochs):
        epoch_start_time = time.time()

        g_loss_epoch = 0.0
        d_loss_epoch = 0.0

        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader),
                           desc=f'Epoch {epoch+1}/{num_epochs}')

        for i, real_images in progress_bar:
            batch_size = real_images.size(0)
            real_images = real_images.to(device)

            # ================================================================
            # Train Discriminator
            # ================================================================
            for _ in range(DISCRIMINATOR_STEPS):
                discriminator.zero_grad()

                # Train on real images
                label_real = torch.full((batch_size, 1), real_label,
                                       dtype=torch.float, device=device)
                output_real = discriminator(real_images)
                loss_real = criterion(output_real, label_real)

                # Train on fake images
                noise = torch.randn(batch_size, LATENT_DIM, 1, 1).to(device)
                fake_images = generator(noise)
                label_fake = torch.full((batch_size, 1), fake_label,
                                       dtype=torch.float, device=device)
                output_fake = discriminator(fake_images.detach())
                loss_fake = criterion(output_fake, label_fake)

                # Total discriminator loss
                d_loss = loss_real + loss_fake
                d_loss.backward()
                optimizer_D.step()

            # ================================================================
            # Train Generator
            # ================================================================
            generator.zero_grad()

            # Generate fake images
            noise = torch.randn(batch_size, LATENT_DIM, 1, 1).to(device)
            fake_images = generator(noise)

            # We want generator to fool discriminator, so we use real labels
            label_real = torch.full((batch_size, 1), 1.0,
                                   dtype=torch.float, device=device)
            output_fake = discriminator(fake_images)
            g_loss = criterion(output_fake, label_real)

            g_loss.backward()
            optimizer_G.step()

            # ================================================================
            # Record losses and update progress
            # ================================================================
            g_losses.append(g_loss.item())
            d_losses.append(d_loss.item())

            g_loss_epoch += g_loss.item()
            d_loss_epoch += d_loss.item()

            # Update progress bar
            progress_bar.set_postfix({
                'D_loss': f'{d_loss.item():.4f}',
                'G_loss': f'{g_loss.item():.4f}',
                'D(x)': f'{output_real.mean().item():.4f}',
                'D(G(z))': f'{output_fake.mean().item():.4f}'
            })

        # ====================================================================
        # End of Epoch
        # ====================================================================
        epoch_time = time.time() - epoch_start_time
        avg_g_loss = g_loss_epoch / len(dataloader)
        avg_d_loss = d_loss_epoch / len(dataloader)

        print(f"\nEpoch [{epoch+1}/{num_epochs}] completed in {epoch_time:.2f}s")
        print(f"Average G Loss: {avg_g_loss:.4f} | Average D Loss: {avg_d_loss:.4f}")

        # Generate and save samples
        if (epoch + 1) % SAMPLE_INTERVAL == 0 or epoch == 0:
            sample_path = f'{SAMPLES_DIR}/epoch_{epoch+1}.png'
            with torch.no_grad():
                fake_samples = generator(fixed_noise)
                fake_samples = (fake_samples + 1) / 2.0
                save_image(fake_samples, sample_path, nrow=8, normalize=False)
            print(f"Samples saved to: {sample_path}")

        # Save checkpoint
        if (epoch + 1) % SAVE_INTERVAL == 0:
            save_checkpoint(epoch + 1, generator, discriminator,
                          optimizer_G, optimizer_D, g_losses, d_losses,
                          CHECKPOINT_DIR)

        print("-" * 70)

    total_time = time.time() - start_time
    print(f"\nTraining completed in {total_time/3600:.2f} hours")

    return g_losses, d_losses

# ============================================================================
# SECTION 8:  TRAINING
# ============================================================================


g_losses, d_losses = train_gan(
    generator, discriminator, dataloader, NUM_EPOCHS,
    optimizer_G, optimizer_D, criterion, device
)

# Plot final losses
plot_losses(g_losses, d_losses, save_path=f'{OUTPUT_DIR}/training_losses.png')

# Generate final samples
generate_samples(generator, num_samples=64, latent_dim=LATENT_DIM,
                 save_path=f'{OUTPUT_DIR}/final_samples.png')

#Save final model
torch.save(generator.state_dict(), f'{OUTPUT_DIR}/generator_final.pt')
torch.save(discriminator.state_dict(), f'{OUTPUT_DIR}/discriminator_final.pt')

print("\n" + "="*70)
print("GENERATION COMPLETE!!")





